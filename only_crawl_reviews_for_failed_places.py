import csv
import asyncio
import os
from playwright.async_api import async_playwright
from service.place_data_collector import crawl_reviews
from storage.save_data import save_reviews_csv

INPUT_DIR = "data/failed_places"  # ìˆ˜ì •ëœ ë””ë ‰í† ë¦¬
OUTPUT_DIR = "data/failed_places"  # ë™ì¼ ê²½ë¡œì— ì €ì¥

async def only_review_crawling_batch():
    files = [f for f in os.listdir(INPUT_DIR) if f.endswith(".csv")]
    if not files:
        print("âŒ ì²˜ë¦¬í•  ì‹¤íŒ¨ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    async with async_playwright() as p:
        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        )

        for filename in files:
            input_path = os.path.join(INPUT_DIR, filename)
            print(f"\nğŸ“‚ ì²˜ë¦¬ ì¤‘: {filename}")

            with open(input_path, newline='', encoding='utf-8-sig') as csvfile:
                reader = csv.DictReader(csvfile)
                rows = list(reader)
                targets = [
                    (row['adm_dong_code'], row['pname'], row['pid'], row)
                    for row in rows if row.get('adm_dong_code') and row.get('pname') and row.get('pid')
                ]

            succeeded_pids = set()

            for adm_dong_code, pname, pid, row in targets:
                page = await context.new_page()
                try:
                    print(f"ğŸ” ë¦¬ë·° ìˆ˜ì§‘ ì‹œì‘: {pname} ({pid})")
                    reviews = await asyncio.wait_for(
                        crawl_reviews(page, pid, pname),
                        timeout=120
                    )
                    if reviews:
                        save_reviews_csv(reviews, adm_dong_code)
                        succeeded_pids.add(pid)
                        print(f"âœ… ë¦¬ë·° ì €ì¥ ì™„ë£Œ: {pname} | ë¦¬ë·° ìˆ˜: {len(reviews)}")
                    else:
                        print(f"âš ï¸ ë¦¬ë·° ì—†ìŒ: {pname}")
                except asyncio.TimeoutError:
                    print(f"[ERROR] Timeout - {pname} ({pid})")
                except Exception as e:
                    print(f"[ERROR] ì˜ˆì™¸ ë°œìƒ - {pname} ({pid}): {e}")
                finally:
                    await page.close()

            # ì‹¤íŒ¨í•œ rowë§Œ ë‹¤ì‹œ ì €ì¥
            failed_rows = [row for _, _, pid, row in targets if pid not in succeeded_pids]
            if failed_rows:
                fieldnames = failed_rows[0].keys()
                failed_path = os.path.join(OUTPUT_DIR, f"failed_places_{failed_rows[0]['adm_dong_code']}.csv")
                with open(failed_path, "w", newline='', encoding='utf-8-sig') as f:
                    writer = csv.DictWriter(f, fieldnames=fieldnames)
                    writer.writeheader()
                    writer.writerows(failed_rows)
                print(f"ğŸ“Œ ì‹¤íŒ¨ {len(failed_rows)}ê°œë¥¼ {failed_path}ì— ì €ì¥ ì™„ë£Œ")
            else:
                print("ğŸ‰ ëª¨ë“  ì¥ì†Œ ë¦¬ë·° ìˆ˜ì§‘ ì„±ê³µ!")

        await browser.close()

if __name__ == "__main__":
    asyncio.run(only_review_crawling_batch())
