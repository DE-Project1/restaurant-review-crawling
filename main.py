import os
import csv
import time
import glob
import asyncio
from crawl_place_info_html import crawl_from_place_ids

RAW_DIR = "raw_data"
PLACE_CSV_DIR = "data/place_info"
ADM_DONG_CODES = ["1111", "1114", "1120"]

from itertools import chain

async def crawl_missing_place_ids():
    os.makedirs(RAW_DIR, exist_ok=True)

    csv_files = list(chain.from_iterable(
        glob.glob(f"{PLACE_CSV_DIR}/place_info_{dong_code}*.csv") for dong_code in ADM_DONG_CODES
    ))

    for csv_path in csv_files:
        global_start = time.time()
        adm_dong_code = os.path.splitext(os.path.basename(csv_path))[0].split("_")[-1]

        place_ids_to_crawl = set()

        # ìˆ˜ì§‘í•´ì•¼ í•  place_id ì°¾ê¸°
        with open(csv_path, newline='', encoding='utf-8-sig') as f:
            reader = csv.DictReader(f)
            for row in reader:
                place_id = row.get("place_id")
                if not place_id:
                    continue
                output_path = f"{RAW_DIR}/adc_{adm_dong_code}_place_rawdata_{place_id}.txt"
                if os.path.exists(output_path):
                    continue  # ì´ë¯¸ ìˆìœ¼ë©´ skip
                place_ids_to_crawl.add(place_id)

        if not place_ids_to_crawl:
            print(f"âœ… {adm_dong_code}: ëª¨ë‘ ìˆ˜ì§‘ë¨ (skip)")
            continue

        # í¬ë¡¤ë§ ì‹¤í–‰
        try:
            print(f"ğŸš€ {adm_dong_code}: {len(place_ids_to_crawl)}ê°œ ìˆ˜ì§‘ ì‹œì‘")
            await crawl_from_place_ids(list(place_ids_to_crawl), RAW_DIR, adm_dong_code)
        except Exception as e:
            print(f"âŒ [ERROR] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")


        global_elapsed = time.time() - global_start
        print(f"âœ… ì „ì²´ ì†Œìš”ì‹œê°„: {global_elapsed:.1f} sec\n")

if __name__ == "__main__":
    asyncio.run(crawl_missing_place_ids())
