import csv
from service.place_searcher import search_and_fetch_place_ids
import asyncio
import os
import time
from playwright.async_api import async_playwright
from storage.save_data import save_place_info_json, save_reviews_json, save_place_raw_html, save_failed_places_json
from service.utils import block_unnecessary_resources, log
from service.place_data_collector import fetch_home_page_and_get_place_info, fetch_review_page_and_get_reviews, fetch_info_page

RAW_DIR="raw_data"
TARGET_TXT_PATH="data/target_adm_dong_codes.txt" # í¬ë¡¤ë§í•  í–‰ì •ë™ì½”ë“œ
ADM_DONG_CSV_PATH="data/adm_dong_list.csv" # í–‰ì •ë™ csv íŒŒì¼
MAX_PLACES=60

# ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜
async def run() -> None:
    # íŒŒì¼ì„ ì…ë ¥ìœ¼ë¡œ ëŒ€ìƒ í–‰ì •ë™ì½”ë“œë¥¼ ì½ì–´ì˜´
    with open(TARGET_TXT_PATH, 'r', encoding='utf-8-sig') as f:
        target_codes = set(line.strip() for line in f if line.strip())

    # í–‰ì •ë™ì½”ë“œë¡œ êµ¬+ë™ ì •ë³´ íšë“
    adm_dong_dict = {}
    with open(ADM_DONG_CSV_PATH, newline='', encoding='utf-8-sig') as csvfile:
        reader = csv.DictReader(csvfile)
        for row in reader:
            adm_dong_code = row["adm_dong_code"]
            if adm_dong_code in target_codes:
                # "êµ¬ ë™" í˜•íƒœë¡œ value êµ¬ì„±
                adm_dong_dict[adm_dong_code] = f"{row["district"]} {row["neighborhood"]}"

    for adm_dong_code in target_codes:
        global_start = time.time()
        # í¬ë¡¤ë§ ì‹¤í–‰ (ì¶«í›„ ì¤‘ë³µ í™•ì¸ ë¡œì§ ì¶”ê°€)
        try:
            place_ids = await search_and_fetch_place_ids(adm_dong_dict[adm_dong_code], MAX_PLACES) # ìµœëŒ€ ìˆ˜ì§‘ ì¥ì†Œ ìˆ˜
            print(f"ğŸš€ {adm_dong_code}: {len(place_ids)}ê°œ ìˆ˜ì§‘ ì‹œì‘")
            await crawl_from_place_ids(list(place_ids), adm_dong_code)

        except Exception as e:
            print(f"âŒ [ERROR] í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜ ë°œìƒ - {e}")

        global_elapsed = time.time() - global_start
        print(f"âœ… ì „ì²´ ì†Œìš”ì‹œê°„: {global_elapsed:.1f} sec\n")


async def crawl_from_place_ids(place_ids: list[str], adm_dong_code):
    results = []
    async with async_playwright() as p:
        place_ids = list(place_ids)

        browser = await p.chromium.launch(headless=False)
        context = await browser.new_context(
            viewport={"width": 400, "height": 800},
            user_agent="Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36"
        )

        # 4ê°œì”© ì²˜ë¦¬
        for i in range(0, len(place_ids), 4):
            batch = place_ids[i:i + 4]

            tasks = []
            for place_id in batch:
                output_path = f"{RAW_DIR}/adc_{adm_dong_code}_place_rawdata_{place_id}.html"
                if os.path.exists(output_path):
                    print(f"â­ï¸ ì´ë¯¸ íŒŒì¼ ìˆìŒ â†’ ìŠ¤í‚µ (PlaceID: {place_id})")
                    continue
                tasks.append(scrape_place_details(context, place_id, adm_dong_code))

            # ë™ì‹œì— ì‹¤í–‰
            batch_results = await asyncio.gather(*tasks, return_exceptions=True)

            # ê²°ê³¼ ì €ì¥
            for result in batch_results:
                if isinstance(result, dict):
                    results.append(result)
        await context.close()
        await browser.close()

    return results

# ì‹ë‹¹ 1ê°œ í¬ë¡¤ë§
async def scrape_place_details(context, place_id: str, adm_dong_code: int):
    log("START", place_id)
    start_time = time.time()
    html_data = {"place_id": place_id, "home_html": None, "info_html": None, "reviews_html": None}

    # íƒ­ ì—´ê¸°
    home_page = await context.new_page()
    info_page = await context.new_page()
    review_page = await context.new_page()

    # ë¦¬ì†ŒìŠ¤ ì°¨ë‹¨
    await block_unnecessary_resources(home_page)
    await block_unnecessary_resources(info_page)
    await block_unnecessary_resources(review_page)

    try:
        # ì„¸ ê°€ì§€ ì‘ì—…ì„ ë³‘ë ¬ë¡œ ì‹¤í–‰. í•˜ë‚˜ë¼ë„ ì˜ˆì™¸ ë°œìƒ ì‹œ ì „ì²´ê°€ ì˜ˆì™¸ë¥¼ ë˜ì§‘ë‹ˆë‹¤.
        place_info, reviews, info_html = await asyncio.gather(
            fetch_home_page_and_get_place_info(home_page, place_id, adm_dong_code),
            fetch_review_page_and_get_reviews(review_page, place_id),
            fetch_info_page(info_page, place_id),
        )
    except Exception as e:
        # ì—¬ê¸°ë¡œ ì˜¤ë©´ place_info, reviews, info_html ì¤‘ í•˜ë‚˜ë¼ë„ ì‹¤íŒ¨í•œ ê²ƒ
        print(f"âŒ {place_id} - ìˆ˜ì§‘ ë¶ˆì¶©ë¶„ â†’ ì €ì¥ ìƒëµ")
        # í•„ìš”í•˜ë‹¤ë©´ ë¦¬ì†ŒìŠ¤ ì •ë¦¬ í›„
        await home_page.close()
        await info_page.close()
        await review_page.close()
        return  # í•¨ìˆ˜ ì „ì²´ ì¢…ë£Œ

    try:
        html_data["home_html"] = await home_page.content()
        html_data["info_html"] = info_html
        html_data["reviews_html"] = await review_page.content()

        save_place_info_json(place_info, adm_dong_code)
        save_reviews_json(reviews, adm_dong_code)
        save_place_raw_html(place_id, adm_dong_code, html_data)

        elapsed = time.time() - start_time
        log("SAVED", place_id, extra=f"{elapsed:.1f} sec")
    except Exception as e:
        save_failed_places_json(place_id, adm_dong_code)
        log("ERROR", place_id, extra=str(e))
    finally: # í˜ì´ì§€ ë‹«ê¸°
        await home_page.close()
        await info_page.close()
        await review_page.close()